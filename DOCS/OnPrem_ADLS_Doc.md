# On-Premises to ADLS Data Pipeline Documentation

## Overview
This document provides a detailed overview of the **On-Premises to ADLS Data Pipeline**, including logging, retry mechanisms, alerting, and other implemented functionalities. The pipeline enables seamless data movement from an on-premises file system to **Azure Data Lake Storage (ADLS)**, ensuring reliability, automation, and monitoring.

---

## **Pipeline Flow**
### 1ï¸âƒ£ **Get File Count (On-Premises) ğŸ—ï¸**
- Uses **Get Metadata Activity** to retrieve the list of files available in the on-premises folder.
- Stores the file names in a pipeline variable for tracking.

### 2ï¸âƒ£ **Append File Count to Variable ğŸ“Š**
- The list of file names obtained from **Get Metadata** is appended to an **Array Variable**.
- This helps track how many files are being processed in the pipeline run.

### 3ï¸âƒ£ **Copy Files from On-Prem to ADLS ğŸ“‚**
- Uses **Copy Activity** to transfer data from the **On-Premises File Server** to **ADLS (Bronze Layer)**.
- Implemented **retry logic** and **error handling** to ensure resilience.

### 4ï¸âƒ£ **Logging Pipeline Execution ğŸ“**
- After copying files, logs are written to **ADLS under `activity-logs/pipeline-logs/`**.
- Captures the following details:
  - `pipelineRunId`
  - `pipelineName`
  - `runTimestamp`

### 5ï¸âƒ£ **Trigger Logic App for Notifications ğŸ””**
- **Web Activity** triggers an **Azure Logic App** for pipeline success/failure notifications.
- The Logic App sends alerts based on execution status.

---

## **Implemented Features âœ…**
### ğŸ“Œ **Retry Logic in Copy Activity**
- Configured **3 retries** with a **30-second interval** to ensure robustness in case of temporary failures.

### ğŸ“Œ **Error Handling & Alerts ğŸš¨**
- If the pipeline fails at any stage, an **alert is triggered via Azure Logic Apps**.
- **Success and failure notifications** are sent to a configured endpoint.

### ğŸ“Œ **Logging & Monitoring ğŸ“**
- Every pipeline run generates logs that are stored in **ADLS**.
- **Log file format:** `pipeline-log-YYYY-MM-DDTHH-MM-SS.csv`.

### ğŸ“Œ **Parameterized Pipelines ğŸ”§**
- Pipeline is **parameterized** for flexibility.
- Parameters include:
  - `source_folder`
  - `file_name`
  - `target_container`
  - `log_container`
  - `log_path`

---

## **Screenshots ğŸ“¸**
Below are key screenshots capturing the pipeline execution, Logic App workflow, and monitoring dashboards.

### âœ… **Logic App Designer (Success & Failure Conditions)**
![Logic App Designer](Logicappdesigner.png)

### âœ… **Logic App Run History (Showing Successful Execution)**
![Logic App Run History](LogicApprunhistory.png)

### âœ… **Azure Data Factory Pipeline Run (Success)**
![ADF Pipeline Run](ADFPipelinerun.png)

### âœ… **Azure Data Factory Log File (CSV Doc preview within ADLS)**
![Log File Created](Logfilecreated.png)

---

## **Pipeline JSON Files ğŸ“œ**
### ğŸ”¹ **Pipeline JSON Configuration:**
- The complete JSON configuration of this pipeline is available in the repository under the **adf_pipelines/** directory.
- All **datasets**, **linked services**, and **parameterized configurations** are structured for reusability and scalability.

---

## **Conclusion ğŸš€**
This pipeline successfully automates the **on-prem to cloud migration**, ensuring:
- **Reliability** through retry logic âœ…
- **Monitoring & alerts** via Logic Apps âœ…
- **Logging & auditability** in ADLS âœ…
